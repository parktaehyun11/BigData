{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## dataframe"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "myList = [\n    ('1','park, aa',190),\n    ('1','lee, bb',195),\n    ('2','lim, cc',200),\n    ('2','lee',195)\n]"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "myDf = spark.createDataFrame(myList)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n |-- _3: long (nullable = true)\n\n"
    }
   ],
   "source": "myDf.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[Row(_1='1', _2='park, aa', _3=190)]\n"
    }
   ],
   "source": "print(myDf.take(1))"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": "myDf_with_columnname = spark.createDataFrame(myList, ['year','name','height'])"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- year: string (nullable = true)\n |-- name: string (nullable = true)\n |-- height: long (nullable = true)\n\n"
    }
   ],
   "source": "myDf_with_columnname.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[Row(year='1', name='park, aa', height=190)]\n"
    }
   ],
   "source": "print(myDf_with_columnname.take(1))"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----+--------+------+\n|year|    name|height|\n+----+--------+------+\n|   1|park, aa|   190|\n|   1| lee, bb|   195|\n|   2| lim, cc|   200|\n|   2|     lee|   195|\n+----+--------+------+\n\n"
    }
   ],
   "source": "myDf_with_columnname.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## row 객체 사용하기"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import Row\nPerson = Row('year','name','height')\nrow1 = Person('1','park, aa',190)\nrow2 = Person('1','lee, bb',195)\nrow3 = Person('2','lim, cc',200)\nrow4 = Person('2','lee',195)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": "myRows = [row1,\n         row2,\n         row3,\n         row4]"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": "myDf_row = spark.createDataFrame(myRows)"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- year: string (nullable = true)\n |-- name: string (nullable = true)\n |-- height: long (nullable = true)\n\n"
    }
   ],
   "source": "myDf_row.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[Row(year='1', name='park, aa', height=190)]\n"
    }
   ],
   "source": "print(myDf_row.take(1))"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----+--------+------+\n|year|    name|height|\n+----+--------+------+\n|   1|park, aa|   190|\n|   1| lee, bb|   195|\n|   2| lim, cc|   200|\n|   2|     lee|   195|\n+----+--------+------+\n\n"
    }
   ],
   "source": "myDf_row.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## schema 정의하고 dataframe 생성하기"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import Row\nPerson = Row('year','name','height')\nrow1 = Person('1','park, aa',190)\nrow2 = Person('1','lee, bb',195)\nrow3 = Person('2','lim, cc',200)\nrow4 = Person('2','lee',195)\n\nmyRows = [row1,\n         row2,\n         row3,\n         row4]"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import StringType, IntegerType\nmySchema=StructType([\n    StructField(\"year\", StringType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"height\", IntegerType(), True)\n])"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": "myDf_schema=spark.createDataFrame(myRows, mySchema)"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- year: string (nullable = true)\n |-- name: string (nullable = true)\n |-- height: integer (nullable = true)\n\n"
    }
   ],
   "source": "myDf_schema.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(year='1', name='park, aa', height=190)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myDf_schema.take(1)"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----+--------+------+\n|year|    name|height|\n+----+--------+------+\n|   1|park, aa|   190|\n|   1| lee, bb|   195|\n|   2| lim, cc|   200|\n|   2|     lee|   195|\n+----+--------+------+\n\n"
    }
   ],
   "source": "myDf_schema.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RDD - schema 자동 인식"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": "myList = [\n    ('1','park, aa',190),\n    ('1','lee, bb',195),\n    ('2','lim, cc',200),\n    ('2','lee',195)\n]"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": "myRdd = spark.sparkContext.parallelize(myList)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## toDF()"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": "rddDf = myRdd.toDF()"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n |-- _3: long (nullable = true)\n\n"
    }
   ],
   "source": "rddDf.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+--------+---+\n| _1|      _2| _3|\n+---+--------+---+\n|  1|park, aa|190|\n|  1| lee, bb|195|\n|  2| lim, cc|200|\n|  2|     lee|195|\n+---+--------+---+\n\n"
    }
   ],
   "source": "rddDf.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## createDataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": "rddDf1 = spark.createDataFrame(myRdd)"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n |-- _3: long (nullable = true)\n\n"
    }
   ],
   "source": "rddDf1.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+--------+---+\n| _1|      _2| _3|\n+---+--------+---+\n|  1|park, aa|190|\n|  1| lee, bb|195|\n|  2| lim, cc|200|\n|  2|     lee|195|\n+---+--------+---+\n\n"
    }
   ],
   "source": "rddDf1.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DBMS와 유사한 dataframe"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+-------+\n| _1|     _2|\n+---+-------+\n|  1|lee, bb|\n|  2|lim, cc|\n|  2|    lee|\n+---+-------+\n\n"
    }
   ],
   "source": "rddDf.where(rddDf._3 >= 195)\\\n     .select([rddDf._1,rddDf._2])\\\n     .show()"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+-------+\n| _1|max(_3)|\n+---+-------+\n|  1|    195|\n|  2|    200|\n+---+-------+\n\n"
    }
   ],
   "source": "rddDf.groupby(rddDf._1).max().show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## schema 정의하고 dataframe 생성하기"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": "myList = [\n    ('1','park, aa',190),\n    ('1','lee, bb',195),\n    ('2','lim, cc',200),\n    ('2','lee',195)\n]"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": "myRdd = spark.sparkContext.parallelize(myList)"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": "schema=StructType([\n    StructField(\"year\", StringType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"height\", IntegerType(), True)\n])"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": "myDf_schema = spark.createDataFrame(myRdd,schema)"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- year: string (nullable = true)\n |-- name: string (nullable = true)\n |-- height: integer (nullable = true)\n\n"
    }
   ],
   "source": "myDf_schema.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----+--------+------+\n|year|    name|height|\n+----+--------+------+\n|   1|park, aa|   190|\n|   1| lee, bb|   195|\n|   2| lim, cc|   200|\n|   2|     lee|   195|\n+----+--------+------+\n\n"
    }
   ],
   "source": "myDf_schema.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Writing data/people.csv\n"
    }
   ],
   "source": "%%writefile data/people.csv\npark, 26\nlee, 28\nkim, 21\nlim, 42"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import Row\ncfile = os.path.join(\"data\",\"people.csv\")\nlines = spark.sparkContext.textFile(cfile)\n\n#sparkContext.textFile() 함수로 읽은 파일은 RDD이다."
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": "parts = lines.map(lambda l: l.split(\",\"))\npeople = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n\n_myDf = spark.createDataFrame(people)\n#RDD에서 Row()를 사용하여 Dataframe으로 변환한다."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
    },
    {
     "data": {
      "text/plain": "[Row(age=26, name='park'),\n Row(age=28, name='lee'),\n Row(age=21, name='kim'),\n Row(age=42, name='lim')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_myDf.printSchema()\n_myDf.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+----+\n|age|name|\n+---+----+\n| 26|park|\n| 28| lee|\n| 21| kim|\n| 42| lim|\n+---+----+\n\n"
    }
   ],
   "source": "_myDf.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DataFrame으로 직접 읽기\n\n- csv 패키지를 사용해서 읽어 본다. 우선 Spark의 csv 패키지를 추가한다. 패키지는 설정파일 spark-defaults.conf에 추가할 수 있다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## csv 파일 읽기\n- csv는 ,로 분리된 파일을 말한다."
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Writing data/ds_spark.csv\n"
    }
   ],
   "source": "%%writefile data/ds_spark.csv\n1,2,3,4\n11,22,33,44\n111,222,333,444"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+\n|  1|  2|  3|  4|\n+---+---+---+---+\n| 11| 22| 33| 44|\n|111|222|333|444|\n+---+---+---+---+\n\n"
    }
   ],
   "source": "df = spark.read.format('com.databricks.spark.csv')\\\n    .options(header='true', inferschema='true').load('data/ds_spark.csv')\ndf.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## tsv 파일 읽기\n- tsv는 Tab으로 분리된 파일을 말한다. '\\t'이 포함되어 있는 경우, Spark는 string으로 데이터타잎을 설정한다."
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.658985, 4.285136])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "import numpy as np\nnp.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.658985, 4.285136])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "import numpy as np\nnp.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Overwriting data/ds_spark_heightweight.txt\n"
    }
   ],
   "source": "%%writefile data/ds_spark_heightweight.txt\n1\t65.78\t112.99\n2\t71.52\t136.49\n3\t69.40\t153.03\n4\t68.22\t142.34\n5\t67.79\t144.30\n6\t68.70\t123.30\n7\t69.80\t141.49\n8\t70.01\t136.46\n9\t67.90\t112.37\n10\t66.78\t120.67\n11\t66.49\t127.45\n12\t67.62\t114.14\n13\t68.30\t125.61\n14\t67.12\t122.46\n15\t68.28\t116.09\n16\t71.09\t140.00\n17\t66.46\t129.50\n18\t68.65\t142.97\n19\t71.23\t137.90\n20\t67.13\t124.04\n21\t67.83\t141.28\n22\t68.88\t143.54\n23\t63.48\t97.90\n24\t68.42\t129.50\n25\t67.63\t141.85\n26\t67.21\t129.72\n27\t70.84\t142.42\n28\t67.49\t131.55\n29\t66.53\t108.33\n30\t65.44\t113.89\n31\t69.52\t103.30\n32\t65.81\t120.75\n33\t67.82\t125.79\n34\t70.60\t136.22\n35\t71.80\t140.10\n36\t69.21\t128.75\n37\t66.80\t141.80\n38\t67.66\t121.23\n39\t67.81\t131.35\n40\t64.05\t106.71\n41\t68.57\t124.36\n42\t65.18\t124.86\n43\t69.66\t139.67\n44\t67.97\t137.37\n45\t65.98\t106.45\n46\t68.67\t128.76\n47\t66.88\t145.68\n48\t67.70\t116.82\n49\t69.82\t143.62\n50\t69.09\t134.93"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.types import *\nrdd=spark.sparkContext\\\n    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n\ntRdd=rdd.map(lambda x:x.split('\\t'))\ntDf=spark.createDataFrame(tRdd)"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n |-- _3: string (nullable = true)\n\n"
    }
   ],
   "source": "tDf.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(_1='1', _2='65.78', _3='112.99')]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "tDf.take(1)"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+-----+------+\n| _1|   _2|    _3|\n+---+-----+------+\n|  1|65.78|112.99|\n|  2|71.52|136.49|\n|  3|69.40|153.03|\n|  4|68.22|142.34|\n|  5|67.79|144.30|\n|  6|68.70|123.30|\n|  7|69.80|141.49|\n|  8|70.01|136.46|\n|  9|67.90|112.37|\n| 10|66.78|120.67|\n| 11|66.49|127.45|\n| 12|67.62|114.14|\n| 13|68.30|125.61|\n| 14|67.12|122.46|\n| 15|68.28|116.09|\n| 16|71.09|140.00|\n| 17|66.46|129.50|\n| 18|68.65|142.97|\n| 19|71.23|137.90|\n| 20|67.13|124.04|\n+---+-----+------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": "tDf.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## withcolumn\n- withColumn()은 열을 추가한다. 기존에 있는 _1행을 integer로 형변환해서 id행을 만들고, 기존의 _1행을 삭제해보자.\n\n- drop()은 열을 삭제할 때 사용한다."
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\ntDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\ntDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+------+------+\n| id|height|weight|\n+---+------+------+\n|  1| 65.78|112.99|\n|  2| 71.52|136.49|\n|  3|  69.4|153.03|\n|  4| 68.22|142.34|\n|  5| 67.79| 144.3|\n|  6|  68.7| 123.3|\n|  7|  69.8|141.49|\n|  8| 70.01|136.46|\n|  9|  67.9|112.37|\n| 10| 66.78|120.67|\n| 11| 66.49|127.45|\n| 12| 67.62|114.14|\n| 13|  68.3|125.61|\n| 14| 67.12|122.46|\n| 15| 68.28|116.09|\n| 16| 71.09| 140.0|\n| 17| 66.46| 129.5|\n| 18| 68.65|142.97|\n| 19| 71.23| 137.9|\n| 20| 67.13|124.04|\n+---+------+------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": "tDf.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 형변환\n- 위 tsv 파일에서 생성한 RDD를 탭으로 분리하면서, float()로 형변환을 해보자."
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[1.0, 65.78, 112.99]]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "import numpy as np\n#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\nrdd=spark.sparkContext.textFile(os.path.join('data','ds_spark_heightweight.txt'))\ntRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\ntRdd.take(1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## dataframe 생성"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": "rdd=spark.sparkContext.textFile(os.path.join('data','ds_spark_heightweight.txt'))\ntRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": "tDf=spark.createDataFrame(tRdd,[\"id\",\"weight\",\"height\"])"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- id: double (nullable = true)\n |-- weight: double (nullable = true)\n |-- height: double (nullable = true)\n\n"
    }
   ],
   "source": "tDf.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----+------+------+\n|  id|weight|height|\n+----+------+------+\n| 1.0| 65.78|112.99|\n| 2.0| 71.52|136.49|\n| 3.0|  69.4|153.03|\n| 4.0| 68.22|142.34|\n| 5.0| 67.79| 144.3|\n| 6.0|  68.7| 123.3|\n| 7.0|  69.8|141.49|\n| 8.0| 70.01|136.46|\n| 9.0|  67.9|112.37|\n|10.0| 66.78|120.67|\n|11.0| 66.49|127.45|\n|12.0| 67.62|114.14|\n|13.0|  68.3|125.61|\n|14.0| 67.12|122.46|\n|15.0| 68.28|116.09|\n|16.0| 71.09| 140.0|\n|17.0| 66.46| 129.5|\n|18.0| 68.65|142.97|\n|19.0| 71.23| 137.9|\n|20.0| 67.13|124.04|\n+----+------+------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": "tDf.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
