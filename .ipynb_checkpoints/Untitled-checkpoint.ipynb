{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_222\"\n",
      "OpenJDK Runtime Environment (Zulu 8.40.0.25-CA-macosx) (build 1.8.0_222-b10)\n",
      "OpenJDK 64-Bit Server VM (Zulu 8.40.0.25-CA-macosx) (build 25.222-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.16\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.24.244:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113db1810>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/Users/park/BigData/spark-warehouse'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.24.244:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일에서 RDD를 생성하기 위해서는 앞서와 같이 SparkContext를 사용한다. 파일명을 textFile() 함수 인자로 넣어서 만들어 주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkContext 들어가므로 RDD 생성\n",
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikipedia'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia',\n",
       " 'Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia', 'Apache Spark is an open source cluster computing framework.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD와 Spark Dataframe를 만드는 함수는 서로 다르다\n",
    "다음에 배우겠지만, file에서 읽는 방식이 RDD와 Dataframe이 서로 다르다. RDD는 sparkContext.textFile(), Dataframe은 read.text()을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='Wikipedia')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='Wikipedia'),\n",
       " Row(value='Apache Spark is an open source cluster computing framework.'),\n",
       " Row(value='아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.'),\n",
       " Row(value='Apache Spark Apache Spark Apache Spark Apache Spark'),\n",
       " Row(value='아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크'),\n",
       " Row(value=\"Originally developed at the University of California, Berkeley's AMPLab,\"),\n",
       " Row(value='the Spark codebase was later donated to the Apache Software Foundation,'),\n",
       " Row(value='which has maintained it since.'),\n",
       " Row(value='Spark provides an interface for programming entire clusters with'),\n",
       " Row(value='implicit data parallelism and fault-tolerance.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe 생성\n",
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print (myDf.first())\n",
    "myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print (type(myDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv에서 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "print(type(myRdd4))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print (type(myList))\n",
    "print (myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "print(type(myRdd5))\n",
    "myRdd5.take(5)\n",
    "#myRdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'],\n",
       " ['40', ' 27'],\n",
       " ['12', ' 38'],\n",
       " ['15', ' 31'],\n",
       " ['21', ' 1'],\n",
       " ['14', ' 19'],\n",
       " ['46', ' 1'],\n",
       " ['10', ' 34'],\n",
       " ['28', ' 3'],\n",
       " ['48', ' 1'],\n",
       " ['16', ' 2'],\n",
       " ['30', ' 3'],\n",
       " ['32', ' 2'],\n",
       " ['48', ' 1'],\n",
       " ['31', ' 2'],\n",
       " ['22', ' 1'],\n",
       " ['12', ' 3'],\n",
       " ['39', ' 29'],\n",
       " ['19', ' 37'],\n",
       " ['25', ' 2']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print (c2f(celsius))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map() 함수를 사용해 보기\n",
    "1) 첫째 인자는 처리함수이고, 함수의 return 값 반드시 있어야 한다\n",
    "\n",
    "2) 두번째 인자는 처리하려는 데이터이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'map'>\n",
      "<map object at 0x11524f7d0>\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print(type(f))\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lambda 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*2\n",
    "y=f(1)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "y=lambda x:x*2\n",
    "print(type(y))\n",
    "print(y(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.56\n",
      "97.7\n",
      "99.14\n",
      "100.03999999999999\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "y=lambda c:((float(9)/5)*c + 32)\n",
    "\n",
    "for i in range (0,len(celsius)):\n",
    "    print(y(celsius[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "a=list(map(lambda c:(float(9)/5)*c + 32, celsius))\n",
    "print(type(a))\n",
    "print(a)\n",
    "\n",
    "#map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "#map(lambda x:x.split(),sentence)\n",
    "a=list(map(lambda x:x.split(),sentence))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'World'], ['Good', 'Morining']]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Hello World\", \"Good Morining\"]\n",
    "#map(lambda x:x.split(),sentence)\n",
    "a=list(map(lambda x:x.split(),sentence))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x%2, fib)\n",
    "result_list = list(result)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[70] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print(type(squared))\n",
    "print (squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myRdd100.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Apache Spark is an open source cluster computing framework.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'Spark provides an interface for programming entire clusters with']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print (\"How many lines having 'Spark': \",myRdd_spark.count())\n",
    "print(type(myRdd_spark))\n",
    "myRdd_spark.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print (myRdd_unicode.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split()).filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "Apache\n",
      "Spark\n",
      "open\n",
      "source\n",
      "cluster\n",
      "computing\n",
      "framework.\n",
      "아파치\n",
      "스파크는\n",
      "오픈\n",
      "소스\n",
      "클러스터\n",
      "컴퓨팅\n",
      "프레임워크이다.\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "Originally\n",
      "developed\n",
      "University\n",
      "of\n",
      "California,\n",
      "Berkeley's\n",
      "AMPLab,\n",
      "Spark\n",
      "codebase\n",
      "was\n",
      "later\n",
      "donated\n",
      "to\n",
      "Apache\n",
      "Software\n",
      "Foundation,\n",
      "which\n",
      "has\n",
      "maintained\n",
      "it\n",
      "since.\n",
      "Spark\n",
      "provides\n",
      "interface\n",
      "programming\n",
      "entire\n",
      "clusters\n",
      "with\n",
      "implicit\n",
      "data\n",
      "parallelism\n",
      "and\n",
      "fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "for words in myRdd_stop.collect():\n",
    "    print(words,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia',\n",
       " 'Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foreach()\n",
    "foreach()는 action이지만, 다른 action 함수들과 달리 반환 값이 없다. 각 요소에 대해 적용한다는 역할에 대해 유사한 기능을 하는 map() 함수가 있다. map() 함수는 각 요소에 대해 계산을 하고, 그 값을 반환한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map 함수로 단어 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.'],\n",
       " ['Apache', 'Spark', 'Apache', 'Spark', 'Apache', 'Spark', 'Apache', 'Spark'],\n",
       " ['아파치', '스파크', '아파치', '스파크', '아파치', '스파크', '아파치', '스파크'],\n",
       " ['Originally',\n",
       "  'developed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'University',\n",
       "  'of',\n",
       "  'California,',\n",
       "  \"Berkeley's\",\n",
       "  'AMPLab,'],\n",
       " ['the',\n",
       "  'Spark',\n",
       "  'codebase',\n",
       "  'was',\n",
       "  'later',\n",
       "  'donated',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Apache',\n",
       "  'Software',\n",
       "  'Foundation,'],\n",
       " ['which', 'has', 'maintained', 'it', 'since.'],\n",
       " ['Spark',\n",
       "  'provides',\n",
       "  'an',\n",
       "  'interface',\n",
       "  'for',\n",
       "  'programming',\n",
       "  'entire',\n",
       "  'clusters',\n",
       "  'with'],\n",
       " ['implicit', 'data', 'parallelism', 'and', 'fault-tolerance.']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=myRdd2.map(lambda x:x.split(\" \"))\n",
    "type(sentences)\n",
    "sentences.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "sentences2=myRdd2.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "\n",
      "-----\n",
      "Apache\n",
      "Spark\n",
      "is\n",
      "an\n",
      "open\n",
      "source\n",
      "cluster\n",
      "computing\n",
      "framework.\n",
      "\n",
      "-----\n",
      "아파치\n",
      "스파크는\n",
      "오픈\n",
      "소스\n",
      "클러스터\n",
      "컴퓨팅\n",
      "프레임워크이다.\n",
      "\n",
      "-----\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "\n",
      "-----\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "\n",
      "-----\n",
      "Originally\n",
      "developed\n",
      "at\n",
      "the\n",
      "University\n",
      "of\n",
      "California,\n",
      "Berkeley's\n",
      "AMPLab,\n",
      "\n",
      "-----\n",
      "the\n",
      "Spark\n",
      "codebase\n",
      "was\n",
      "later\n",
      "donated\n",
      "to\n",
      "the\n",
      "Apache\n",
      "Software\n",
      "Foundation,\n",
      "\n",
      "-----\n",
      "which\n",
      "has\n",
      "maintained\n",
      "it\n",
      "since.\n",
      "\n",
      "-----\n",
      "Spark\n",
      "provides\n",
      "an\n",
      "interface\n",
      "for\n",
      "programming\n",
      "entire\n",
      "clusters\n",
      "with\n",
      "\n",
      "-----\n",
      "implicit\n",
      "data\n",
      "parallelism\n",
      "and\n",
      "fault-tolerance.\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.collect():\n",
    "    for word in line:\n",
    "        print (word,)\n",
    "    print (\"\\n-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Apache Spark is an open source cluster computing framework\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'a line']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['a', 'line']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print (upperRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
