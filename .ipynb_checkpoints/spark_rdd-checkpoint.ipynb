{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/private/var/folders/gy/180hq9612vs8x1m58v9gc8bh0000gn/T/spark-864ceb28-66cf-43ac-966f-24fa2891256b/userFiles-18c8e8c7-d162-4bb8-89d6-da9e80292a9c\n/Users/park/BigData\n/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/py4j-0.10.7-src.zip\n/usr/local/Cellar/apache-spark/2.4.4/libexec/python\n/Users/park/BigData\n/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip\n/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7\n/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload\n\n/usr/local/lib/python3.7/site-packages\n/usr/local/lib/python3.7/site-packages/IPython/extensions\n/Users/park/.ipython\n"
    }
   ],
   "source": "import sys\n\nfor i in sys.path:\n    print (i)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RDD"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "myList=[1,2,3,4,5,6,7]\nmyRdd1 = spark.sparkContext.parallelize(myList)"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd1.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd1.take(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 파일에서 RDD 생성"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Overwriting data/ds_spark_wiki.txt\n"
    }
   ],
   "source": "%%writefile data/ds_spark_wiki.txt\nWikipedia\nApache Spark is an open source cluster computing framework.\n아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\nApache Spark Apache Spark Apache Spark Apache Spark\n아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\nOriginally developed at the University of California, Berkeley's AMPLab,\nthe Spark codebase was later donated to the Apache Software Foundation,\nwhich has maintained it since.\nSpark provides an interface for programming entire clusters with\nimplicit data parallelism and fault-tolerance."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# csv에서 RDD 생성"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Overwriting data/ds_spark_2cols.csv\n"
    }
   ],
   "source": "%%writefile data/ds_spark_2cols.csv\n35, 2\n40, 27\n12, 38\n15, 31\n21, 1\n14, 19\n46, 1\n10, 34\n28, 3\n48, 1\n16, 2\n30, 3\n32, 2\n48, 1\n31, 2\n22, 1\n12, 3\n39, 29\n19, 37"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": "myRdd4 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd4.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pyspark.rdd.RDD'>\n<class 'list'>\n"
    }
   ],
   "source": "print (type(myRdd4))\nmyList=myRdd4.take(5)\nprint (type(myList))"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd5 = myRdd4.map(lambda line: line.split(','))\nmyRdd5.take(5)"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[102.56, 97.7, 99.14, 100.03999999999999]\n"
    }
   ],
   "source": "celsius = [39.2, 36.5, 37.3, 37.8]\ndef c2f(c):\n    return (float(9)/5)*c + 32\nf=map(c2f, celsius)\nmyList = list(f)\nprint (myList)"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\n"
    }
   ],
   "source": "y=lambda x:x*2\nprint (y(1))"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[102.56, 97.7, 99.14, 100.03999999999999]\n"
    }
   ],
   "source": "celsius = [39.2, 36.5, 37.3, 37.8]\nmyList1 = list(map(lambda x:(float(9)/5)*x + 32,celsius))\nprint (myList1)"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]\n"
    }
   ],
   "source": "sentence =\"Hello World\"\nmyList2=list(map(lambda x:x.split(),sentence))\nprint(myList2)"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['Hello', 'World'], ['Good', 'Morning']]\n"
    }
   ],
   "source": "sentence = [\"Hello World\",\"Good Morning\"]\nmyList3 = list(map(lambda x:x.split(),sentence))\nprint (myList3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# filter()"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1, 1, 3, 5, 13, 21, 55]\n"
    }
   ],
   "source": "fib = [0,1,1,2,3,5,8,13,21,34,55]\nresult = list(filter(lambda x: x % 2, fib))\nprint (result)"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "How many lines having 'Spark':  4\n"
    }
   ],
   "source": "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\nprint (\"How many lines having 'Spark': \",myRdd_spark.count())"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\n"
    },
    {
     "data": {},
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\nprint (myRdd_unicode.count())\nmyRdd_unicode.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# stopwords"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": "stopwords = ['is','am','are','the','for','a', 'an', 'at']\nmyRdd_stop = myRdd2.flatMap(lambda x:x.split()).filter(lambda x: x not in stopwords)"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd_stop.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Wikipedia\nApache\nSpark\nopen\nsource\ncluster\ncomputing\nframework.\n아파치\n스파크는\n오픈\n소스\n클러스터\n컴퓨팅\n프레임워크이다.\nApache\nSpark\nApache\nSpark\nApache\nSpark\nApache\nSpark\n아파치\n스파크\n아파치\n스파크\n아파치\n스파크\n아파치\n스파크\nOriginally\ndeveloped\nUniversity\nof\nCalifornia,\nBerkeley's\nAMPLab,\nSpark\ncodebase\nwas\nlater\ndonated\nto\nApache\nSoftware\nFoundation,\nwhich\nhas\nmaintained\nit\nsince.\nSpark\nprovides\ninterface\nprogramming\nentire\nclusters\nwith\nimplicit\ndata\nparallelism\nand\nfault-tolerance.\n"
    }
   ],
   "source": "for words in myRdd_stop.collect():\n    print (words),"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# reduce()"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "from functools import reduce\nreduce(lambda x, y: x+y, range(1,101))"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PythonRDD[15] at RDD at PythonRDD.scala:53\n"
    },
    {
     "data": {},
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\nsquared = nRdd.map(lambda x: x * x)\nprint (squared)\nsquared.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd100 = spark.sparkContext.parallelize(range(1,101))\nmyRdd100.reduce(lambda x,y: x+y)"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sum:  10\nmin:  1\nmax:  4\nstandard deviation: 1.118033988749895\nvariance:  1.25\n"
    }
   ],
   "source": "print (\"sum: \",nRdd.sum())\nprint (\"min: \",nRdd.min())\nprint (\"max: \", nRdd.max())\nprint (\"standard deviation:\", nRdd.stdev())\nprint (\"variance: \", nRdd.variance())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# foreach()"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": "def f(x): print(x)\nspark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# map 함수로 단어 분리"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": "sentences=myRdd2.map(lambda x:x.split(\" \"))"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sentences.count()"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PythonRDD[36] at RDD at PythonRDD.scala:53\n"
    }
   ],
   "source": "print(sentences)\n#sentences.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": "def mySplit(x):\n    return x.split(\" \")\nsentences2=myRdd2.map(mySplit)"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sentences2.count()"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.map(lambda s:len(s)).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['this', 'is'], ['a', 'line']]\n"
    }
   ],
   "source": "myList=[\"this is\",\"a line\"]\n_rdd=spark.sparkContext.parallelize(myList)\nwordsRdd=_rdd.map(lambda x:x.split())\nprint (wordsRdd.collect())"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n#repRdd.take(10)\nrepRdd.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['THIS', 'A']\n"
    }
   ],
   "source": "upperRDD =wordsRdd.map(lambda x: x[0].upper())\nprint (upperRDD.collect())"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['THIS', 'IS'], ['A', 'LINE']]\n"
    }
   ],
   "source": "upper2RDD =wordsRdd.map(lambda x: [i.upper() for i in x])\nprint (upper2RDD.collect())"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2, 2]\n"
    }
   ],
   "source": "wordsLength = wordsRdd.map(len).collect()\nprint (wordsLength)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 파일에 쓰기"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n           (\"key1\",1),(\"key2\",1),\n           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n_testRdd=spark.sparkContext.parallelize(_testList)"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.groupBy(lambda x:x[0]).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.groupBy(lambda x:x[0]).mapValues(list).collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# pair rdd"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n           (\"key1\",1),(\"key2\",1),\n           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n_testRdd=spark.sparkContext.parallelize(_testList)"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.keys().collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.groupByKey().collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_testRdd.groupByKey().mapValues(list).collect() # list is a function, that is, list()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 단어빈도 예제"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2\\\n    .flatMap(lambda x:x.split())\\\n    .map(lambda x:(x,1))\\\n    .groupByKey()\\\n    .collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2\\\n    .flatMap(lambda x:x.split())\\\n    .map(lambda x:(x,1))\\\n    .groupByKey()\\\n    .mapValues(sum)\\\n    .take(10)"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('AMPLab,', 1)\n('Apache', 6)\n(\"Berkeley's\", 1)\n('California,', 1)\n('Foundation,', 1)\n('Originally', 1)\n('Software', 1)\n('Spark', 7)\n('University', 1)\n('Wikipedia', 1)\n"
    }
   ],
   "source": "def f(x): return len(x)\n# myRdd2\\\n#     .flatMap(lambda x:x.split())\\\n#     .map(lambda x:(x,1))\\\n#     .groupByKey()\\\n#     .mapValues(f)\\\n#     .sortByKey(True)\\\n#     .take(10)\nwc = myRdd2\\\n    .flatMap(lambda x:x.split())\\\n    .map(lambda x:(x,1))\\\n    .groupByKey()\\\n    .mapValues(f)\\\n    .sortByKey(True)\\\n    .take(10)\nfor e in wc:\n    print (e)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": "myRdd3=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_bigdata_wiki.txt\"))\n"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Big data\n활용사례 및 의의[편집]\n"
    }
   ],
   "source": "for i in myRdd3.take(2):\n    print (i)"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": "wc3=myRdd3\\\n    .flatMap(lambda x:x.split(\" \"))\\\n    .take(10)"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Big\ndata\n활용사례\n및\n의의[편집]\n정치\n및\n사회[편집]\n2008년\n미국\n"
    }
   ],
   "source": "for i in wc3:\n    print (i),"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": "wc3=myRdd3\\\n    .map(lambda x:x.split(\" \"))\\\n    .take(3)"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Big', 'data']\n['활용사례', '및', '의의[편집]']\n['정치', '및', '사회[편집]']\n"
    }
   ],
   "source": "for i in wc3:\n    print (i),"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['Big', 'data'], ['활용사례', '및', '의의[편집]'], ['정치', '및', '사회[편집]']]\n"
    }
   ],
   "source": "print (wc3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# stopwords 제거"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": "stopwords = [u'및','big','is','am','are','the','for','a']\nwc3_stop1 = myRdd3\\\n    .flatMap(lambda x: x.split(' '))\\\n    .filter(lambda x: x.lower() not in stopwords)\\\n    .take(10)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data\n활용사례\n의의[편집]\n정치\n사회[편집]\n2008년\n미국\n대통령\n선거[편집]\n2008년\n"
    }
   ],
   "source": "for i in wc3_stop1:\n    print (i)"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('Big data', 1)\n('활용사례 및 의의[편집]', 1)\n('정치 및 사회[편집]', 1)\n"
    }
   ],
   "source": "wc3=myRdd3\\\n    .map(lambda x:(x,1))\\\n    .take(3)\nfor i in wc3:\n    print (i)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "myRdd2\\\n    .flatMap(lambda x:x.split())\\\n    .map(lambda x:(x,1))\\\n    .groupByKey()\\\n    .mapValues(sum)\\\n    .take(10)"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": "stopwords = [u'및',u'등', u'수', 'big','is','am','are','the','for','a', 'an','the']\nwc3=myRdd3\\\n    .flatMap(lambda x:x.split(\" \"))\\\n    .filter(lambda x: x.lower() not in stopwords)\\\n    .map(lambda x:(x,1))\\\n    .reduceByKey(lambda x,y:x+y)\\\n    .map(lambda x:(x[1],x[0]))\\\n    .sortByKey(False)\\\n    .take(15)"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'list'>\n23 \n21 데이터\n18 데이터를\n14 빅\n9 있다.\n8 데이터의\n7 미국\n7 통해\n6 유권자\n6 대한\n6 선거\n6 빅데이터\n5 활용한\n5 소셜\n5 있는\n"
    }
   ],
   "source": "print (type(wc3))\nfor i in wc3:\n    print (i[0],i[1])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RDD 생성"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": "marks=[\n    \"'김하나','English', 100\",\n    \"'김하나','Math', 80\",\n    \"'임하나','English', 70\",\n    \"'임하나','Math', 100\",\n    \"'김갑돌','English', 82.3\",\n    \"'김갑돌','Math', 98.5\"\n]\n_marksRdd=spark.sparkContext.parallelize(marks)"
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_marksRdd.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 문제 3-1: 이름으로 합계를 구해보자. 올바른 출력은 다음과 같다."
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'임하나' 170.0\n'김하나' 180.0\n'김갑돌' 180.8\n"
    }
   ],
   "source": "_marksbyname=_marksRdd\\\n    .map(lambda x:x.split(','))\\\n    .map(lambda x: (x[0],float(x[2])))\\\n    .reduceByKey(lambda x,y:x+y)\\\n    .collect()\nfor i in _marksbyname:\n    print (i[0],i[1])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 문제 3-2: 과목으로 합계를 계산해 보자. 출력은 다음과 같이 나와야 한다."
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'Math' 278.5\n'English' 252.3\n"
    }
   ],
   "source": "# marks by subject\n_marksbysubject=_marksRdd\\\n    .map(lambda x:x.split(','))\\\n    .map(lambda x: (x[1],float(x[2])))\\\n    .reduceByKey(lambda x,y:x+y)\\\n    .collect()\nfor i in _marksbysubject:\n    print (i[0],i[1])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 문제 3-3: 이름으로 합계과 개수를 구해보자. 출력은 다음과 같이 계산된다.\n'임하나' (170.0, 2)\n\n'김하나' (180.0, 2)\n\n'김갑돌' (180.8, 2)"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": "# marks by name\n_marksbyname2=_marksRdd\\\n    .map(lambda x:x.split(','))\\\n    .map(lambda x: (x[0],float(x[2])))"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": "# sum, counts by name\nsum_counts = _marksbyname2.combineByKey(\n    (lambda x: (x, 1)), # the initial value, with value x and count 1\n    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sum_counts.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'임하나'(170.0, 2)\n'김하나'(180.0, 2)\n'김갑돌'(180.8, 2)\n"
    }
   ],
   "source": "for i in sum_counts.collect():\n    for each in i:\n        print (each,end='')\n    print ()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 문제 3-4: 이름으로 평균을 계산해 보자. 앞서 3-3에서 사용했던 결과를 활용하고, 올바른 출력은 다음과 같다."
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-177-b279a0fc80ed>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-177-b279a0fc80ed>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    .map(lambda key,(sum1,count)):(key,float(sum1)/count)\\\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": "# average\naverageByKey = sum_counts\\\n    .map(lambda key,(sum1,count)):(key,float(sum1)/count)\\\n    .collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": "averageByKey = sum_counts.mapValues(lambda x:x[0]/x[1]).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'임하나'85.0\n'김하나'90.0\n'김갑돌'90.4\n"
    }
   ],
   "source": "for i in averageByKey:\n    for j in i:\n        print (j,end='')\n    print ()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 전철"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": "_sub=[\"20150101,2호선,0236,영등포구청,6199,6219\",\n\"20150101,2호선,0237,당산,7982,8946\",\n\"20150101,2호선,0238,합정,17406,15241\",\n\"20150101,3호선,0309,지축,515,538\",\n\"20150101,3호선,0310,구파발,6879,6260\",\n\"20150101,3호선,0311,연신내,20031,19470\",\n\"20150101,3호선,0312,불광,9519,11029\",\n\"20150101,4호선,0425,회현,7465,7574\",\n\"20150101,4호선,0426,서울역,3943,10823\",\n\"20150101,경부선,1002,남영,4340,4535\",\n\"20150101,경부선,1003,용산,28980,27684\",\n\"20150101,경부선,1004,노량진,23021,23862\",\n\"20150101,경부선,1005,대방,6360,6476\",\n\"20150101,경부선,1006,영등포,37247,36102\",\n\"20150101,경원선,1008,이촌,1940,1507\",\n\"20150101,경원선,1009,서빙고,911,1000\",\n\"20150101,경원선,1010,한남,1885,1863\",\n\"20150101,경원선,1011,옥수,43,37\"]"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": "_subRdd = spark.sparkContext.parallelize(_sub)"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "_subRdd.map(lambda x:x.split(',')).map(lambda x:int(x[4])).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": "_subLineByPassengers=_subRdd.map(lambda x:x.split(',')).map(lambda x: (x[1],int(x[4])))\nsum_counts1 = _subLineByPassengers.combineByKey(\n    (lambda x: (x, 1)), # the initial value, with value x and count 1\n    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2호선(31587, 3)\n3호선(36944, 4)\n4호선(11408, 2)\n경원선(4779, 4)\n경부선(99948, 5)\n"
    }
   ],
   "source": "for i in sum_counts1.collect():\n    for each in i:\n        print (each,end='')\n    print ()"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2호선 31587 3 10529.0\n3호선 36944 4 9236.0\n4호선 11408 2 5704.0\n경원선 4779 4 1194.75\n경부선 99948 5 19989.6\n"
    }
   ],
   "source": "for i in sum_counts1.collect():\n    print (i[0],i[1][0],i[1][1],i[1][0]/i[1][1])\n    "
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": "#averageByKey = sum_counts.map(lambda (key,(sum,count)):(key,float(sum)/count))\n#averageByKey = sum_counts.map(lambda(key[0],key[1][0],key[1][1]):(key[0],key[1][0]/key[1][1])\naverageByKey = sum_counts1.mapValues(lambda x:x[0]/x[1]).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2호선 10529.0 \n3호선 9236.0 \n4호선 5704.0 \n경원선 1194.75 \n경부선 19989.6 \n"
    }
   ],
   "source": "for i in averageByKey:\n    for j in i:\n        print (j,end=' ')\n    print ()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 단어"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "wc = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n    .flatMap(lambda x: x.split(' '))\\\n    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wc.sortByKey().take(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "wcReduceByKey = wc.reduceByKey(lambda x,y:x+y)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wcReduceByKey.sortByKey().take(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": "wcGroupByKey = wc.groupByKey().mapValues(sum)"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wcGroupByKey.sortByKey().take(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3"
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-197-348a2a66b55e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-197-348a2a66b55e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wcGroupByKey2 = wc.groupByKey().map(lambda(x,v):(x,len(v)))\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": "wcGroupByKey2 = wc.groupByKey().map(lambda(x,v):(x,len(v)))"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wcGroupByKey2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-95c9a9fbce97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwcGroupByKey2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wcGroupByKey2' is not defined"
     ]
    }
   ],
   "source": "wcGroupByKey2.sortByKey().take(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": "wc = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n    .map(lambda x:x.split())\\\n    .map(lambda x:[(i,1) for i in x])"
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('wikipedia', 1)]\n[('apache', 1), ('spark', 1), ('is', 1), ('an', 1), ('open', 1), ('source', 1), ('cluster', 1), ('computing', 1), ('framework', 1)]\n[('아파치', 1), ('스파크는', 1), ('오픈', 1), ('소스', 1), ('클러스터', 1), ('컴퓨팅', 1), ('프레임워크이다', 1)]\n[('apache', 1), ('spark', 1), ('apache', 1), ('spark', 1), ('apache', 1), ('spark', 1), ('apache', 1), ('spark', 1)]\n[('아파치', 1), ('스파크', 1), ('아파치', 1), ('스파크', 1), ('아파치', 1), ('스파크', 1), ('아파치', 1), ('스파크', 1)]\n[('originally', 1), ('developed', 1), ('at', 1), ('the', 1), ('university', 1), ('of', 1), ('california', 1), (\"berkeley's\", 1), ('amplab', 1)]\n[('the', 1), ('spark', 1), ('codebase', 1), ('was', 1), ('later', 1), ('donated', 1), ('to', 1), ('the', 1), ('apache', 1), ('software', 1), ('foundation', 1)]\n[('which', 1), ('has', 1), ('maintained', 1), ('it', 1), ('since', 1)]\n[('spark', 1), ('provides', 1), ('an', 1), ('interface', 1), ('for', 1), ('programming', 1), ('entire', 1), ('clusters', 1), ('with', 1)]\n[('implicit', 1), ('data', 1), ('parallelism', 1), ('and', 1), ('fault', 1), ('tolerance', 1)]\n"
    }
   ],
   "source": "for e in wc.collect():\n    print (e)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
