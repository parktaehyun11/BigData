{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": "import pyspark"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "openjdk version \"1.8.0_222\"\nOpenJDK Runtime Environment (Zulu 8.40.0.25-CA-macosx) (build 1.8.0_222-b10)\nOpenJDK 64-Bit Server VM (Zulu 8.40.0.25-CA-macosx) (build 25.222-b10, mixed mode)\n"
    }
   ],
   "source": "!java -version"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"
    }
   ],
   "source": "!scala -version"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Python 2.7.16\r\n"
    }
   ],
   "source": "!python --version"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "#spark = pyspark.sql.SparkSession.builder.getOrCreate()\nspark"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "spark.conf.get('spark.sql.warehouse.dir')"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "myList=[1,2,3,4,5,6,7]\nmyRdd1 = spark.sparkContext.parallelize(myList)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd1.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd1.take(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 파일에서 RDD 생성하기"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Writing data/ds_spark_wiki.txt\n"
    }
   ],
   "source": "%%writefile data/ds_spark_wiki.txt\nWikipedia\nApache Spark is an open source cluster computing framework.\n아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\nApache Spark Apache Spark Apache Spark Apache Spark\n아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\nOriginally developed at the University of California, Berkeley's AMPLab,\nthe Spark codebase was later donated to the Apache Software Foundation,\nwhich has maintained it since.\nSpark provides an interface for programming entire clusters with\nimplicit data parallelism and fault-tolerance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "파일에서 RDD를 생성하기 위해서는 앞서와 같이 SparkContext를 사용한다. 파일명을 textFile() 함수 인자로 넣어서 만들어 주면 된다."
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": "#sparkContext 들어가므로 RDD 생성\nmyRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.first()"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.take(2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RDD와 Spark Dataframe를 만드는 함수는 서로 다르다\n다음에 배우겠지만, file에서 읽는 방식이 RDD와 Dataframe이 서로 다르다. RDD는 sparkContext.textFile(), Dataframe은 read.text()을 사용한다."
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Row(value='Wikipedia')\n"
    },
    {
     "data": {},
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "#dataframe 생성\nimport os\nmyDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\nprint (myDf.first())\nmyDf.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n"
    }
   ],
   "source": "print (type(myDf))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# csv에서 RDD 생성"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Writing data/ds_spark_2cols.csv\n"
    }
   ],
   "source": "%%writefile data/ds_spark_2cols.csv\n35, 2\n40, 27\n12, 38\n15, 31\n21, 1\n14, 19\n46, 1\n10, 34\n28, 3\n48, 1\n16, 2\n30, 3\n32, 2\n48, 1\n31, 2\n22, 1\n12, 3\n39, 29\n19, 37\n25, 2"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pyspark.rdd.RDD'>\n"
    },
    {
     "data": {},
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd4 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\nprint(type(myRdd4))\nmyRdd4.take(5)"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'list'>\n['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']\n"
    }
   ],
   "source": "myList=myRdd4.take(5)\nprint (type(myList))\nprint (myList)"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pyspark.rdd.PipelinedRDD'>\n"
    },
    {
     "data": {},
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd5 = myRdd4.map(lambda line: line.split(','))\nprint(type(myRdd5))\nmyRdd5.take(5)\n#myRdd5.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd5.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# python"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[102.56, 97.7, 99.14, 100.03999999999999]\n"
    }
   ],
   "source": "celsius = [39.2, 36.5, 37.3, 37.8]\ndef c2f(c):\n    f=list()\n    for i in c:\n        _f=(float(9)/5)*i + 32\n        f.append(_f)\n    return f\n\nprint (c2f(celsius))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# map() 함수를 사용해 보기\n1) 첫째 인자는 처리함수이고, 함수의 return 값 반드시 있어야 한다\n\n2) 두번째 인자는 처리하려는 데이터이다"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'map'>\n<map object at 0x11524f7d0>\n"
    }
   ],
   "source": "celsius = [39.2, 36.5, 37.3, 37.8]\ndef c2f(c):\n    return (float(9)/5)*c + 32\n\nf=map(c2f, celsius)\nprint(type(f))\nprint(f)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# lambda 사용"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\n"
    }
   ],
   "source": "def f(x):\n    return x*2\ny=f(1)\nprint (y)"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'function'>\n4\n"
    }
   ],
   "source": "y=lambda x:x*2\nprint(type(y))\nprint(y(2))"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "102.56\n97.7\n99.14\n100.03999999999999\n"
    }
   ],
   "source": "celsius = [39.2, 36.5, 37.3, 37.8]\ny=lambda c:((float(9)/5)*c + 32)\n\nfor i in range (0,len(celsius)):\n    print(y(celsius[i]))"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'list'>\n[102.56, 97.7, 99.14, 100.03999999999999]\n"
    }
   ],
   "source": "celsius = [39.2, 36.5, 37.3, 37.8]\na=list(map(lambda c:(float(9)/5)*c + 32, celsius))\nprint(type(a))\nprint(a)\n\n#map(lambda c:(float(9)/5)*c + 32, celsius)"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]\n"
    }
   ],
   "source": "sentence = \"Hello World\"\n#map(lambda x:x.split(),sentence)\na=list(map(lambda x:x.split(),sentence))\nprint(a)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['Hello', 'World'], ['Good', 'Morining']]\n"
    }
   ],
   "source": "sentence = [\"Hello World\", \"Good Morining\"]\n#map(lambda x:x.split(),sentence)\na=list(map(lambda x:x.split(),sentence))\nprint(a)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## filter()"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1, 1, 3, 5, 13, 21, 55]\n"
    }
   ],
   "source": "fib = [0,1,1,2,3,5,8,13,21,34,55]\nresult = filter(lambda x: x%2, fib)\nresult_list = list(result)\nprint(result_list)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## reduce()"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "from functools import reduce\nreduce(lambda x, y: x+y, range(1,101))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RDD 사용하기 - 비구조적인것 분석할때 사용"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pyspark.rdd.PipelinedRDD'>\nPythonRDD[70] at RDD at PythonRDD.scala:53\n"
    }
   ],
   "source": "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\nsquared = nRdd.map(lambda x: x * x)\nprint(type(squared))\nprint (squared)"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "squared.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd100 = spark.sparkContext.parallelize(range(1,101))\nmyRdd100.reduce(lambda x,y: x+y)"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": "#myRdd100.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# filter()"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "How many lines having 'Spark':  4\n<class 'pyspark.rdd.PipelinedRDD'>\n"
    },
    {
     "data": {},
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\nprint (\"How many lines having 'Spark': \",myRdd_spark.count())\nprint(type(myRdd_spark))\nmyRdd_spark.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
    }
   ],
   "source": "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\nprint (myRdd_unicode.first())"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": "stopwords = ['is','am','are','the','for','a', 'an', 'at']\nmyRdd_stop = myRdd2.flatMap(lambda x:x.split()).filter(lambda x: x not in stopwords)"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Wikipedia\nApache\nSpark\nopen\nsource\ncluster\ncomputing\nframework.\n아파치\n스파크는\n오픈\n소스\n클러스터\n컴퓨팅\n프레임워크이다.\nApache\nSpark\nApache\nSpark\nApache\nSpark\nApache\nSpark\n아파치\n스파크\n아파치\n스파크\n아파치\n스파크\n아파치\n스파크\nOriginally\ndeveloped\nUniversity\nof\nCalifornia,\nBerkeley's\nAMPLab,\nSpark\ncodebase\nwas\nlater\ndonated\nto\nApache\nSoftware\nFoundation,\nwhich\nhas\nmaintained\nit\nsince.\nSpark\nprovides\ninterface\nprogramming\nentire\nclusters\nwith\nimplicit\ndata\nparallelism\nand\nfault-tolerance.\n"
    }
   ],
   "source": "for words in myRdd_stop.collect():\n    print(words,)"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# foreach()\nforeach()는 action이지만, 다른 action 함수들과 달리 반환 값이 없다. 각 요소에 대해 적용한다는 역할에 대해 유사한 기능을 하는 map() 함수가 있다. map() 함수는 각 요소에 대해 계산을 하고, 그 값을 반환한다.\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def f(x): print(x)\nspark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# map 함수로 단어 분리"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sentences=myRdd2.map(lambda x:x.split(\" \"))\ntype(sentences)\nsentences.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sentences.count()"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": "def mySplit(x):\n    return x.split(\" \")\n\nsentences2=myRdd2.map(mySplit)"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sentences2.count()"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sentences.take(3)"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Wikipedia\n\n-----\nApache\nSpark\nis\nan\nopen\nsource\ncluster\ncomputing\nframework.\n\n-----\n아파치\n스파크는\n오픈\n소스\n클러스터\n컴퓨팅\n프레임워크이다.\n\n-----\nApache\nSpark\nApache\nSpark\nApache\nSpark\nApache\nSpark\n\n-----\n아파치\n스파크\n아파치\n스파크\n아파치\n스파크\n아파치\n스파크\n\n-----\nOriginally\ndeveloped\nat\nthe\nUniversity\nof\nCalifornia,\nBerkeley's\nAMPLab,\n\n-----\nthe\nSpark\ncodebase\nwas\nlater\ndonated\nto\nthe\nApache\nSoftware\nFoundation,\n\n-----\nwhich\nhas\nmaintained\nit\nsince.\n\n-----\nSpark\nprovides\nan\ninterface\nfor\nprogramming\nentire\nclusters\nwith\n\n-----\nimplicit\ndata\nparallelism\nand\nfault-tolerance.\n\n-----\n"
    }
   ],
   "source": "for line in sentences.collect():\n    for word in line:\n        print (word,)\n    print (\"\\n-----\")"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "len(\"Apache Spark is an open source cluster computing framework\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd2.map(lambda s:len(s)).collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myList=[\"this is\",\"a line\"]\n_rdd=spark.sparkContext.parallelize(myList)\n_rdd.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "wordsRdd=_rdd.map(lambda x:x.split())\nwordsRdd.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "'s'.upper()"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\nrepRdd.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['THIS', 'A']\n"
    }
   ],
   "source": "upperRDD =wordsRdd.map(lambda x: x[0].upper())\nprint (upperRDD.collect())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
