{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\ndv = np.array([1.0, 2.1, 3])"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1.0,2.1,3.0] <class 'pyspark.mllib.linalg.DenseVector'>\n"
    }
   ],
   "source": "from pyspark.mllib.linalg import Vectors\n\ndv1mllib=Vectors.dense([1.0, 2.1, 3])\nprint (dv1mllib, type(dv1mllib))"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1.0,2.1,3.0]\n"
    }
   ],
   "source": "from pyspark.ml.linalg import Vectors\n\ndv1ml=Vectors.dense([1.0, 2.1, 3])\nprint (dv1ml)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1.0\n2.1\n3.0\n"
    }
   ],
   "source": "for e in dv1ml:\n    print (e)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "14.41\n"
    }
   ],
   "source": "print (dv1ml.dot(dv1ml))"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "import numpy as np\nnp.dot(dv,dv)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1.0,4.41,9.0]\n"
    }
   ],
   "source": "print (dv1ml*dv1ml)"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0. 1. 3.]\n"
    }
   ],
   "source": "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\nprint (sv1.toArray())"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(1.0,[1.0,2.0,3.0])\n"
    }
   ],
   "source": "from pyspark.mllib.regression import LabeledPoint\nprint (LabeledPoint(1.0,[1.0,2.0,3.0]))"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(1992.0,(10,[0,1,2],[3.0,5.5,10.0]))\n"
    }
   ],
   "source": "from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\n\nprint (LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0})))"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "from pyspark.mllib.regression import LabeledPoint\n\nLabeledPoint(1.0, dv1mllib)"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\n\nLabeledPoint(1.0, Vectors.fromML(dv1ml))"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\ntrainDf=spark.createDataFrame(p)\ntrainDf.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n     LabeledPoint(1,[1.1,2.1,3.1]),\n     LabeledPoint(0,[1.2,2.2,3.3])]\ntrainDf=spark.createDataFrame(p)\ntrainDf.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "from pyspark.mllib.linalg import Vectors\n\ntrainDf = spark.createDataFrame([\n    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\ntrainDf.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.mllib.linalg import SparseVector, VectorUDT\nfrom pyspark.sql.types import StructType, StructField, DoubleType\n_rdd = spark.sparkContext.parallelize([\n    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])\n\nschema = StructType([\n    StructField(\"label\", DoubleType(), True),\n    StructField(\"features\", VectorUDT(), True)\n])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
