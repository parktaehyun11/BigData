{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Overwriting data/names_with_header.csv\n"
    }
   ],
   "source": "%%writefile data/names_with_header.csv\nID,name,position,state,PC_type\n10,Andrew,Manager,DE,PC\n11,Arun,Manager,NJ,PC\n12,Harish,Sales,NJ,MAC\n13,Robert,Manager,PA,MAC\n14,Laura,Engineer,PA,MAC"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Overwriting data/names_without_header.csv\n"
    }
   ],
   "source": "%%writefile data/names_without_header.csv\n10,Andrew,Manager,DE,PC\n11,Arun,Manager,NJ,PC\n12,Harish,Sales,NJ,MAC\n13,Robert,Manager,PA,MAC\n14,Laura,Engineer,PA,MAC"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1단계:원시 데이터 스파크 RDD로 불러온다"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import HiveContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\n#원시 데이터 스파크 RDD로 불러온다\nmyRdd = sc.textFile(\"/Users/park/BigData/data/names_without_header.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['10,Andrew,Manager,DE,PC', '11,Arun,Manager,NJ,PC', '12,Harish,Sales,NJ,MAC', '13,Robert,Manager,PA,MAC', '14,Laura,Engineer,PA,MAC']\n<class 'pyspark.rdd.RDD'>\n"
    }
   ],
   "source": "print(myRdd.collect())\nprint(type(myRdd))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2단계:RDD를 생성하는 스파크의 map() 함수 이용해서 csv 데이터를 분리한다"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['10', 'Andrew', 'Manager', 'DE', 'PC'], ['11', 'Arun', 'Manager', 'NJ', 'PC'], ['12', 'Harish', 'Sales', 'NJ', 'MAC'], ['13', 'Robert', 'Manager', 'PA', 'MAC'], ['14', 'Laura', 'Engineer', 'PA', 'MAC']]\n"
    }
   ],
   "source": "#RDD를 생성하는 스파크의 map() 함수 이용해서 csv 데이터를 분리한다\nmyRdd1 = myRdd.map(lambda x:x.split(','))\nprint(myRdd1.collect())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3단계:스키마 생성"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": "#내 스키마 생성\nmySchema=StructType([\n    StructField(\"EmployeeID\", StringType(), True),\n    StructField(\"FirstName\", StringType(), True),\n    StructField(\"Title\", StringType(), True),\n    StructField(\"State\", StringType(), True),\n    StructField(\"Laptop\", StringType(), True),\n])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4단계:스키마에 맞춰서 rdd를 dataframe으로 변환"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": "# 스키마에 맞춰서 rdd를 dataframe으로 변환\nmyDf = spark.createDataFrame(myRdd1,mySchema)"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- EmployeeID: string (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- Title: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Laptop: string (nullable = true)\n\n"
    }
   ],
   "source": "myDf.printSchema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 또 다른 방법1"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[['10', 'Andrew', 'Manager', 'DE', 'PC'],\n ['11', 'Arun', 'Manager', 'NJ', 'PC'],\n ['12', 'Harish', 'Sales', 'NJ', 'MAC'],\n ['13', 'Robert', 'Manager', 'PA', 'MAC'],\n ['14', 'Laura', 'Engineer', 'PA', 'MAC']]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myRdd = sc.textFile(\"/Users/park/BigData/data/names_without_header.csv\")\nmyRdd1 = myRdd.map(lambda x:x.split(','))\nmyRdd1.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": "temp = myRdd1.map(lambda x: Row(EmployeeID=int(x[0]), FirstName = x[1]\\\n                                , Title = x[2],State = x[3],Laptop = x[4]))"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": "myDf1 = temp.toDF()"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- EmployeeID: long (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- Laptop: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Title: string (nullable = true)\n\n"
    },
    {
     "data": {
      "text/plain": "[Row(EmployeeID=10, FirstName='Andrew', Laptop='PC', State='DE', Title='Manager'),\n Row(EmployeeID=11, FirstName='Arun', Laptop='PC', State='NJ', Title='Manager'),\n Row(EmployeeID=12, FirstName='Harish', Laptop='MAC', State='NJ', Title='Sales'),\n Row(EmployeeID=13, FirstName='Robert', Laptop='MAC', State='PA', Title='Manager'),\n Row(EmployeeID=14, FirstName='Laura', Laptop='MAC', State='PA', Title='Engineer')]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "myDf1.printSchema()\nmyDf1.collect()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 또 다른 방법2"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": "myDf2 = spark.read.format('com.databricks.spark.csv')\\\n        .options(header='false', infraschema='true').load('data/names_without_header.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+------+--------+---+---+\n|_c0|   _c1|     _c2|_c3|_c4|\n+---+------+--------+---+---+\n| 10|Andrew| Manager| DE| PC|\n| 11|  Arun| Manager| NJ| PC|\n| 12|Harish|   Sales| NJ|MAC|\n| 13|Robert| Manager| PA|MAC|\n| 14| Laura|Engineer| PA|MAC|\n+---+------+--------+---+---+\n\n"
    }
   ],
   "source": "myDf2.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": "#컬럼명 변경\nmyDf2 = myDf2.withColumnRenamed(\"_c0\",\"EmployeeID\")\\\n            .withColumnRenamed(\"_c1\",\"FirstName\")\\\n            .withColumnRenamed(\"_c2\",\"Title\")\\\n            .withColumnRenamed(\"_c3\",\"State\")\\\n            .withColumnRenamed(\"_c4\",\"Laptop\")\\"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----------+---------+--------+-----+------+\n|EmployeeID|FirstName|   Title|State|Laptop|\n+----------+---------+--------+-----+------+\n|        10|   Andrew| Manager|   DE|    PC|\n|        11|     Arun| Manager|   NJ|    PC|\n|        12|   Harish|   Sales|   NJ|   MAC|\n|        13|   Robert| Manager|   PA|   MAC|\n|        14|    Laura|Engineer|   PA|   MAC|\n+----------+---------+--------+-----+------+\n\n"
    }
   ],
   "source": "myDf2.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----------+---------+\n|EmployeeID|FirstName|\n+----------+---------+\n|        10|   Andrew|\n|        11|     Arun|\n+----------+---------+\n\n"
    }
   ],
   "source": "myDf2.select([myDf2.EmployeeID, myDf2.FirstName]).\\\nwhere(myDf2.Laptop == 'PC').show()"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------+-----+\n|Laptop|count|\n+------+-----+\n|    PC|    2|\n|   MAC|    3|\n+------+-----+\n\n"
    }
   ],
   "source": "myDf2.groupby('Laptop').count().show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5단계:생성 dataframe을 하이브 테이블에 저장"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": "#생성 dataframe을 하이브 테이블에 저장\nfrom pyspark.sql import HiveContext\nhc = HiveContext(sc)\nmyDf.write.format(\"orc\").saveAsTable(\"employees_ex\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
