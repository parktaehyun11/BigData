{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Python 2.7.16\r\n"
    }
   ],
   "source": "!python --version"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "openjdk version \"1.8.0_222\"\r\nOpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_222-b10)\r\nOpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.222-b10, mixed mode)\r\n"
    }
   ],
   "source": "!java -version"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"
    }
   ],
   "source": "!scala -version"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Welcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\r\n      /_/\r\n                        \r\nUsing Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_222\r\nBranch \r\nCompiled by user  on 2019-08-27T21:21:38Z\r\nRevision \r\nUrl \r\nType --help for more information.\r\n"
    }
   ],
   "source": "!pyspark --version"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as nop\nimport pandas as pd\nimport os\nfrom pyspark.sql.functions import lit, col\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "- age - age in years\n- sex - (1 = male; 0 = female)\n- cp - chest pain type\n- trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n- chol - serum cholestoral in mg/dl\n- fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n- restecg - resting electrocardiographic results\n- thalach - maximum heart rate achieved\n- exang - exercise induced angina (1 = yes; 0 = no)\n- oldpeak - ST depression induced by exercise relative to rest\n- slope - the slope of the peak exercise ST segment\n- ca - number of major vessels (0-3) colored by flourosopy\n- thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n- target - have disease or not (1=yes, 0=no)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- age: integer (nullable = true)\n |-- sex: integer (nullable = true)\n |-- cp: integer (nullable = true)\n |-- trestbps: integer (nullable = true)\n |-- chol: integer (nullable = true)\n |-- fbs: integer (nullable = true)\n |-- restecg: integer (nullable = true)\n |-- thalach: integer (nullable = true)\n |-- exang: integer (nullable = true)\n |-- oldpeak: double (nullable = true)\n |-- slope: integer (nullable = true)\n |-- ca: integer (nullable = true)\n |-- thal: integer (nullable = true)\n |-- target: integer (nullable = true)\n\n"
    }
   ],
   "source": "_trainDf.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "_trainDf = spark.read.format('com.databricks.spark.csv')\\\n    .options(header='true', inferschema='true')\\\n    .load(os.path.join(\"data\",\"kaggle\",\"heartattack\",\"heart.csv\"))\ndf = _trainDf\ndf = df.withColumn(\"target1\",_trainDf['target'].cast(\"double\")).drop('target') #target column double형으로 변경\ndf = df.drop('cp','thal','slope') # 건강지표와 필요 없는 column drop\ntrain,test = df.randomSplit([0.5,0.5],seed=11) #데이터 반으로 랜덤하게 train, test 데이터로 나눔"
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----------+-----+\n|testOrtrain|count|\n+-----------+-----+\n|      train|  153|\n|       test|  150|\n+-----------+-----+\n\n"
    }
   ],
   "source": "train = train.withColumn('testOrtrain',lit('train')) #train data인것 알려주기 위해서 새로운 column 생성해서 train이라고 알려줌 \ntest = test.withColumn('testOrtFrain',lit('test')) #test data인것 알려주기 위해서 새로운 column 생성해서 test라고 알려줌\ndf = train.select('age','sex','trestbps','chol','fbs','restecg','thalach',\\\n                  'exang','oldpeak','ca','target1','testOrtrain')\\\n            .union(test.select('age','sex','trestbps','chol','fbs','restecg',\\\n                               'thalach','exang','oldpeak','ca','target1','testOrtrain'))\ndf.groupBy('testOrtrain').count().show()"
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": "va = VectorAssembler(inputCols=[\"age\",\"sex\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\\\n                                \"exang\",\"oldpeak\",\"ca\"], outputCol = \"features\") # feature 벡터 구성\npipeline = Pipeline(stages=[va]) #파이프라인 구성\nmodel = pipeline.fit(df) #fit 이용해서 모델 만들기\nmyDf = model.transform(df)"
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": "train=myDf.filter(myDf['testOrtrain']=='train')\ntestDf=myDf.filter(myDf['testOrtrain']=='test')\ntrainDf,validateDf = train.randomSplit([0.7,0.3],seed=11)"
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": "lr = LogisticRegression().\\\n    setLabelCol('target1').\\\n    setFeaturesCol('features').\\\n    setRegParam(0.0).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.)"
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": "lrModel=lr.fit(trainDf)"
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": "lrDf = lrModel.transform(validateDf)"
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----------+-------+-----+\n|prediction|target1|count|\n+----------+-------+-----+\n|       1.0|    1.0|   20|\n|       0.0|    1.0|    2|\n|       1.0|    0.0|    8|\n|       0.0|    0.0|   12|\n+----------+-------+-----+\n\n"
    }
   ],
   "source": "lrDf.groupBy('prediction','target1').count().show()"
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "75.45454545454547 %\n"
    }
   ],
   "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction',labelCol='target1')\nprint(evaluator.evaluate(lrDf)*100 ,\"%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
