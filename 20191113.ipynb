{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "tf = 1./4\n",
    "print (tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=float(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.09861228866811"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log((N+1)/(df+1))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when': 24, 'find': 4, 'myself': 15, 'in': 7, 'times': 21, 'of': 16, 'trouble': 23, 'mother': 13, 'mary': 11, 'comes': 2, 'to': 22, 'me': 12, 'speaking': 19, 'words': 27, 'wisdom': 26, 'let': 10, 'it': 9, 'be': 1, 'and': 0, 'my': 14, 'hour': 6, 'darkness': 3, 'she': 18, 'is': 8, 'standing': 20, 'right': 17, 'front': 5, 'whisper': 25}\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.791759469228055\n",
      "  (0, 9)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 13)\t2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                        |\n",
      "|나 Let it be                          |\n",
      "|너 Let it be                          |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]\n",
    "\n",
    "myDf=spark.createDataFrame(doc,['sent'])\n",
    "myDf.show(truncate=False) #truncate 너무 길면 자른다. false이면 자르지 않고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      9.0|\n",
      "|Mother Mary comes...|      8.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      5.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|      우리 Let it be|      6.0|\n",
      "|        나 Let it be|      1.0|\n",
      "|        너 Let it be|      2.0|\n",
      "|           Let it be|      7.0|\n",
      "|Whisper words of ...|      3.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent='When I find myself in times of trouble', words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'])\n",
      "Row(sent='Mother Mary comes to me', words=['mother', 'mary', 'comes', 'to', 'me'])\n",
      "Row(sent='Speaking words of wisdom, let it be', words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)\n",
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------------------------------------+\n",
      "|sent                                  |wordsReg                                       |\n",
      "+--------------------------------------+-----------------------------------------------+\n",
      "|When I find myself in times of trouble|[when, i, find, myself, in, times, of, trouble]|\n",
      "|Mother Mary comes to me               |[mother, mary, comes, to, me]                  |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |\n",
      "|And in my hour of darkness            |[and, in, my, hour, of, darkness]              |\n",
      "|She is standing right in front of me  |[she, is, standing, right, in, front, of, me]  |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |\n",
      "|우리 Let it be                        |[우리, let, it, be]                            |\n",
      "|나 Let it be                          |[나, let, it, be]                              |\n",
      "|너 Let it be                          |[너, let, it, be]                              |\n",
      "|Let it be                             |[let, it, be]                                  |\n",
      "|Whisper words of wisdom, let it be    |[whisper, words, of, wisdom,, let, it, be]     |\n",
      "+--------------------------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")\n",
    "reDf=re.transform(myDf)\n",
    "reDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_87853317e176"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now i'll you'll he'll she'll we'll they'll i'd you'd he'd she'd we'd they'd i'm you're he's she's it's we're they're i've we've you've they've isn't aren't wasn't weren't haven't hasn't hadn't don't doesn't didn't won't wouldn't shan't shouldn't mustn't can't couldn't cannot could here's how's let's ought that's there's what's when's where's who's why's would 나 너 우리 "
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[4,7,9],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[6,8,10],[1.0...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[13,14],[1.0,...|\n",
      "|She is standing r...|[standing, right,...|(16,[11,12,15],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|      우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,5],[1....|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "let wisdom, words speaking times whisper mother trouble comes find mary right standing hour darkness front "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\",\n",
    "    vocabSize=30,minDF=1.0)\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.collect()\n",
    "cvDf.select('sent','nostops','cv').show()\n",
    "for v in cvModel.vocabulary:\n",
    "    print (v,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=['find', 'times', 'trouble'], hash=SparseVector(50, {10: 1.0, 24: 1.0, 43: 1.0}))\n",
      "Row(nostops=['mother', 'mary', 'comes'], hash=SparseVector(50, {1: 1.0, 21: 1.0, 24: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=['hour', 'darkness'], hash=SparseVector(50, {23: 1.0, 27: 1.0}))\n",
      "Row(nostops=['standing', 'right', 'front'], hash=SparseVector(50, {24: 1.0, 43: 1.0, 46: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(50, {14: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=50)\n",
    "hashDf = hashTF.transform(stopDf)\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)\n",
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([0.0632, 0.0406, -0.0583]))\n",
      "Row(w2v=DenseVector([0.0218, -0.0456, -0.0727]))\n",
      "Row(w2v=DenseVector([0.0067, -0.0311, -0.0142]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3,minCount=0,inputCol=\"words\",outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)\n",
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|                sent|               words|                ngrams|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|When I find mysel...|[when, i, find, m...|  [when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|  [mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|  [and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|  [she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|      우리 Let it be| [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|        나 Let it be|   [나, let, it, be]| [나 let, let it, i...|\n",
      "|        너 Let it be|   [너, let, it, be]| [너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|       [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|  [whisper words, w...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "\n",
      "Row(words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'], ngrams=['when i', 'i find', 'find myself', 'myself in', 'in times', 'times of', 'of trouble'])\n",
      "Row(words=['mother', 'mary', 'comes', 'to', 'me'], ngrams=['mother mary', 'mary comes', 'comes to', 'to me'])\n",
      "Row(words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'], ngrams=['speaking words', 'words of', 'of wisdom,', 'wisdom, let', 'let it', 'it be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1, 65.78, 112.99\n",
    "2, 71.52, 136.49\n",
    "3, 69.40, 153.03\n",
    "4, 68.22, 142.34\n",
    "5, 67.79, 144.30\n",
    "6, 68.70, 123.30\n",
    "7, 69.80, 141.49\n",
    "8, 70.01, 136.46\n",
    "9, 67.90, 112.37\n",
    "10, 66.78, 120.67\n",
    "11, 66.49, 127.45\n",
    "12, 67.62, 114.14\n",
    "13, 68.30, 125.61\n",
    "14, 67.12, 122.46\n",
    "15, 68.28, 116.09\n",
    "16, 71.09, 140.00\n",
    "17, 66.46, 129.50\n",
    "18, 68.65, 142.97\n",
    "19, 71.23, 137.90\n",
    "20, 67.13, 124.04\n",
    "21, 67.83, 141.28\n",
    "22, 68.88, 143.54\n",
    "23, 63.48, 97.90\n",
    "24, 68.42, 129.50\n",
    "25, 67.63, 141.85\n",
    "26, 67.21, 129.72\n",
    "27, 70.84, 142.42\n",
    "28, 67.49, 131.55\n",
    "29, 66.53, 108.33\n",
    "30, 65.44, 113.89\n",
    "31, 69.52, 103.30\n",
    "32, 65.81, 120.75\n",
    "33, 67.82, 125.79\n",
    "34, 70.60, 136.22\n",
    "35, 71.80, 140.10\n",
    "36, 69.21, 128.75\n",
    "37, 66.80, 141.80\n",
    "38, 67.66, 121.23\n",
    "39, 67.81, 131.35\n",
    "40, 64.05, 106.71\n",
    "41, 68.57, 124.36\n",
    "42, 65.18, 124.86\n",
    "43, 69.66, 139.67\n",
    "44, 67.97, 137.37\n",
    "45, 65.98, 106.45\n",
    "46, 68.67, 128.76\n",
    "47, 66.88, 145.68\n",
    "48, 67.70, 116.82\n",
    "49, 69.82, 143.62\n",
    "50, 69.09, 134.93\n",
    "51, 69.91, 147.02\n",
    "52, 67.33, 126.33\n",
    "53, 70.27, 125.48\n",
    "54, 69.10, 115.71\n",
    "55, 65.38, 123.49\n",
    "56, 70.18, 147.89\n",
    "57, 70.41, 155.90\n",
    "58, 66.54, 128.07\n",
    "59, 66.36, 119.37\n",
    "60, 67.54, 133.81\n",
    "61, 66.50, 128.73\n",
    "62, 69.00, 137.55\n",
    "63, 68.30, 129.76\n",
    "64, 67.01, 128.82\n",
    "65, 70.81, 135.32\n",
    "66, 68.22, 109.61\n",
    "67, 69.06, 142.47\n",
    "68, 67.73, 132.75\n",
    "69, 67.22, 103.53\n",
    "70, 67.37, 124.73\n",
    "71, 65.27, 129.31\n",
    "72, 70.84, 134.02\n",
    "73, 69.92, 140.40\n",
    "74, 64.29, 102.84\n",
    "75, 68.25, 128.52\n",
    "76, 66.36, 120.30\n",
    "77, 68.36, 138.60\n",
    "78, 65.48, 132.96\n",
    "79, 69.72, 115.62\n",
    "80, 67.73, 122.52\n",
    "81, 68.64, 134.63\n",
    "82, 66.78, 121.90\n",
    "83, 70.05, 155.38\n",
    "84, 66.28, 128.94\n",
    "85, 69.20, 129.10\n",
    "86, 69.13, 139.47\n",
    "87, 67.36, 140.89\n",
    "88, 70.09, 131.59\n",
    "89, 70.18, 121.12\n",
    "90, 68.23, 131.51\n",
    "91, 68.13, 136.55\n",
    "92, 70.24, 141.49\n",
    "93, 71.49, 140.61\n",
    "94, 69.20, 112.14\n",
    "95, 70.06, 133.46\n",
    "96, 70.56, 131.80\n",
    "97, 66.29, 120.03\n",
    "98, 63.43, 123.10\n",
    "99, 66.77, 128.14\n",
    "100, 68.89, 115.48\n",
    "101, 64.87, 102.09\n",
    "102, 67.09, 130.35\n",
    "103, 68.35, 134.18\n",
    "104, 65.61, 98.64\n",
    "105, 67.76, 114.56\n",
    "106, 68.02, 123.49\n",
    "107, 67.66, 123.05\n",
    "108, 66.31, 126.48\n",
    "109, 69.44, 128.42\n",
    "110, 63.84, 127.19\n",
    "111, 67.72, 122.06\n",
    "112, 70.05, 127.61\n",
    "113, 70.19, 131.64\n",
    "114, 65.95, 111.90\n",
    "115, 70.01, 122.04\n",
    "116, 68.61, 128.55\n",
    "117, 68.81, 132.68\n",
    "118, 69.76, 136.06\n",
    "119, 65.46, 115.94\n",
    "120, 68.83, 136.90\n",
    "121, 65.80, 119.88\n",
    "122, 67.21, 109.01\n",
    "123, 69.42, 128.27\n",
    "124, 68.94, 135.29\n",
    "125, 67.94, 106.86\n",
    "126, 65.63, 123.29\n",
    "127, 66.50, 109.51\n",
    "128, 67.93, 119.31\n",
    "129, 68.89, 140.24\n",
    "130, 70.24, 133.98\n",
    "131, 68.27, 132.58\n",
    "132, 71.23, 130.70\n",
    "133, 69.10, 115.56\n",
    "134, 64.40, 123.79\n",
    "135, 71.10, 128.14\n",
    "136, 68.22, 135.96\n",
    "137, 65.92, 116.63\n",
    "138, 67.44, 126.82\n",
    "139, 73.90, 151.39\n",
    "140, 69.98, 130.40\n",
    "141, 69.52, 136.21\n",
    "142, 65.18, 113.40\n",
    "143, 68.01, 125.33\n",
    "144, 68.34, 127.58\n",
    "145, 65.18, 107.16\n",
    "146, 68.26, 116.46\n",
    "147, 68.57, 133.84\n",
    "148, 64.50, 112.89\n",
    "149, 68.71, 130.76\n",
    "150, 68.89, 137.76\n",
    "151, 69.54, 125.40\n",
    "152, 67.40, 138.47\n",
    "153, 66.48, 120.82\n",
    "154, 66.01, 140.15\n",
    "155, 72.44, 136.74\n",
    "156, 64.13, 106.11\n",
    "157, 70.98, 158.96\n",
    "158, 67.50, 108.79\n",
    "159, 72.02, 138.78\n",
    "160, 65.31, 115.91\n",
    "161, 67.08, 146.29\n",
    "162, 64.39, 109.88\n",
    "163, 69.37, 139.05\n",
    "164, 68.38, 119.90\n",
    "165, 65.31, 128.31\n",
    "166, 67.14, 127.24\n",
    "167, 68.39, 115.23\n",
    "168, 66.29, 124.80\n",
    "169, 67.19, 126.95\n",
    "170, 65.99, 111.27\n",
    "171, 69.43, 122.61\n",
    "172, 67.97, 124.21\n",
    "173, 67.76, 124.65\n",
    "174, 65.28, 119.52\n",
    "175, 73.83, 139.30\n",
    "176, 66.81, 104.83\n",
    "177, 66.89, 123.04\n",
    "178, 65.74, 118.89\n",
    "179, 65.98, 121.49\n",
    "180, 66.58, 119.25\n",
    "181, 67.11, 135.02\n",
    "182, 65.87, 116.23\n",
    "183, 66.78, 109.17\n",
    "184, 68.74, 124.22\n",
    "185, 66.23, 141.16\n",
    "186, 65.96, 129.15\n",
    "187, 68.58, 127.87\n",
    "188, 66.59, 120.92\n",
    "189, 66.97, 127.65\n",
    "190, 68.08, 101.47\n",
    "191, 70.19, 144.99\n",
    "192, 65.52, 110.95\n",
    "193, 67.46, 132.86\n",
    "194, 67.41, 146.34\n",
    "195, 69.66, 145.59\n",
    "196, 65.80, 120.84\n",
    "197, 66.11, 115.78\n",
    "198, 68.24, 128.30\n",
    "199, 68.02, 127.47\n",
    "200, 71.39, 127.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext.textFile(os.path.join('/Users/park/BigData/data/ds_spark_heightweight.txt'))\n",
    "myRdd = rdd.map(lambda line:[float(x) for x in line.split(',')])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[97] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda line:[float(x) for x in line.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.textFile(os.path.join('/Users/park/BigData/data/ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Index\", \"Height(Inches)\", \"Weight(Pounds)\"',\n",
       " '1, 65.78, 112.99',\n",
       " '2, 71.52, 136.49',\n",
       " '3, 69.40, 153.03',\n",
       " '4, 68.22, 142.34',\n",
       " '5, 67.79, 144.30',\n",
       " '6, 68.70, 123.30',\n",
       " '7, 69.80, 141.49',\n",
       " '8, 70.01, 136.46',\n",
       " '9, 67.90, 112.37',\n",
       " '10, 66.78, 120.67',\n",
       " '11, 66.49, 127.45',\n",
       " '12, 67.62, 114.14',\n",
       " '13, 68.30, 125.61',\n",
       " '14, 67.12, 122.46',\n",
       " '15, 68.28, 116.09',\n",
       " '16, 71.09, 140.00',\n",
       " '17, 66.46, 129.50',\n",
       " '18, 68.65, 142.97',\n",
       " '19, 71.23, 137.90',\n",
       " '20, 67.13, 124.04',\n",
       " '21, 67.83, 141.28',\n",
       " '22, 68.88, 143.54',\n",
       " '23, 63.48, 97.90',\n",
       " '24, 68.42, 129.50',\n",
       " '25, 67.63, 141.85',\n",
       " '26, 67.21, 129.72',\n",
       " '27, 70.84, 142.42',\n",
       " '28, 67.49, 131.55',\n",
       " '29, 66.53, 108.33',\n",
       " '30, 65.44, 113.89',\n",
       " '31, 69.52, 103.30',\n",
       " '32, 65.81, 120.75',\n",
       " '33, 67.82, 125.79',\n",
       " '34, 70.60, 136.22',\n",
       " '35, 71.80, 140.10',\n",
       " '36, 69.21, 128.75',\n",
       " '37, 66.80, 141.80',\n",
       " '38, 67.66, 121.23',\n",
       " '39, 67.81, 131.35',\n",
       " '40, 64.05, 106.71',\n",
       " '41, 68.57, 124.36',\n",
       " '42, 65.18, 124.86',\n",
       " '43, 69.66, 139.67',\n",
       " '44, 67.97, 137.37',\n",
       " '45, 65.98, 106.45',\n",
       " '46, 68.67, 128.76',\n",
       " '47, 66.88, 145.68',\n",
       " '48, 67.70, 116.82',\n",
       " '49, 69.82, 143.62',\n",
       " '50, 69.09, 134.93',\n",
       " '51, 69.91, 147.02',\n",
       " '52, 67.33, 126.33',\n",
       " '53, 70.27, 125.48',\n",
       " '54, 69.10, 115.71',\n",
       " '55, 65.38, 123.49',\n",
       " '56, 70.18, 147.89',\n",
       " '57, 70.41, 155.90',\n",
       " '58, 66.54, 128.07',\n",
       " '59, 66.36, 119.37',\n",
       " '60, 67.54, 133.81',\n",
       " '61, 66.50, 128.73',\n",
       " '62, 69.00, 137.55',\n",
       " '63, 68.30, 129.76',\n",
       " '64, 67.01, 128.82',\n",
       " '65, 70.81, 135.32',\n",
       " '66, 68.22, 109.61',\n",
       " '67, 69.06, 142.47',\n",
       " '68, 67.73, 132.75',\n",
       " '69, 67.22, 103.53',\n",
       " '70, 67.37, 124.73',\n",
       " '71, 65.27, 129.31',\n",
       " '72, 70.84, 134.02',\n",
       " '73, 69.92, 140.40',\n",
       " '74, 64.29, 102.84',\n",
       " '75, 68.25, 128.52',\n",
       " '76, 66.36, 120.30',\n",
       " '77, 68.36, 138.60',\n",
       " '78, 65.48, 132.96',\n",
       " '79, 69.72, 115.62',\n",
       " '80, 67.73, 122.52',\n",
       " '81, 68.64, 134.63',\n",
       " '82, 66.78, 121.90',\n",
       " '83, 70.05, 155.38',\n",
       " '84, 66.28, 128.94',\n",
       " '85, 69.20, 129.10',\n",
       " '86, 69.13, 139.47',\n",
       " '87, 67.36, 140.89',\n",
       " '88, 70.09, 131.59',\n",
       " '89, 70.18, 121.12',\n",
       " '90, 68.23, 131.51',\n",
       " '91, 68.13, 136.55',\n",
       " '92, 70.24, 141.49',\n",
       " '93, 71.49, 140.61',\n",
       " '94, 69.20, 112.14',\n",
       " '95, 70.06, 133.46',\n",
       " '96, 70.56, 131.80',\n",
       " '97, 66.29, 120.03',\n",
       " '98, 63.43, 123.10',\n",
       " '99, 66.77, 128.14',\n",
       " '100, 68.89, 115.48',\n",
       " '101, 64.87, 102.09',\n",
       " '102, 67.09, 130.35',\n",
       " '103, 68.35, 134.18',\n",
       " '104, 65.61, 98.64',\n",
       " '105, 67.76, 114.56',\n",
       " '106, 68.02, 123.49',\n",
       " '107, 67.66, 123.05',\n",
       " '108, 66.31, 126.48',\n",
       " '109, 69.44, 128.42',\n",
       " '110, 63.84, 127.19',\n",
       " '111, 67.72, 122.06',\n",
       " '112, 70.05, 127.61',\n",
       " '113, 70.19, 131.64',\n",
       " '114, 65.95, 111.90',\n",
       " '115, 70.01, 122.04',\n",
       " '116, 68.61, 128.55',\n",
       " '117, 68.81, 132.68',\n",
       " '118, 69.76, 136.06',\n",
       " '119, 65.46, 115.94',\n",
       " '120, 68.83, 136.90',\n",
       " '121, 65.80, 119.88',\n",
       " '122, 67.21, 109.01',\n",
       " '123, 69.42, 128.27',\n",
       " '124, 68.94, 135.29',\n",
       " '125, 67.94, 106.86',\n",
       " '126, 65.63, 123.29',\n",
       " '127, 66.50, 109.51',\n",
       " '128, 67.93, 119.31',\n",
       " '129, 68.89, 140.24',\n",
       " '130, 70.24, 133.98',\n",
       " '131, 68.27, 132.58',\n",
       " '132, 71.23, 130.70',\n",
       " '133, 69.10, 115.56',\n",
       " '134, 64.40, 123.79',\n",
       " '135, 71.10, 128.14',\n",
       " '136, 68.22, 135.96',\n",
       " '137, 65.92, 116.63',\n",
       " '138, 67.44, 126.82',\n",
       " '139, 73.90, 151.39',\n",
       " '140, 69.98, 130.40',\n",
       " '141, 69.52, 136.21',\n",
       " '142, 65.18, 113.40',\n",
       " '143, 68.01, 125.33',\n",
       " '144, 68.34, 127.58',\n",
       " '145, 65.18, 107.16',\n",
       " '146, 68.26, 116.46',\n",
       " '147, 68.57, 133.84',\n",
       " '148, 64.50, 112.89',\n",
       " '149, 68.71, 130.76',\n",
       " '150, 68.89, 137.76',\n",
       " '151, 69.54, 125.40',\n",
       " '152, 67.40, 138.47',\n",
       " '153, 66.48, 120.82',\n",
       " '154, 66.01, 140.15',\n",
       " '155, 72.44, 136.74',\n",
       " '156, 64.13, 106.11',\n",
       " '157, 70.98, 158.96',\n",
       " '158, 67.50, 108.79',\n",
       " '159, 72.02, 138.78',\n",
       " '160, 65.31, 115.91',\n",
       " '161, 67.08, 146.29',\n",
       " '162, 64.39, 109.88',\n",
       " '163, 69.37, 139.05',\n",
       " '164, 68.38, 119.90',\n",
       " '165, 65.31, 128.31',\n",
       " '166, 67.14, 127.24',\n",
       " '167, 68.39, 115.23',\n",
       " '168, 66.29, 124.80',\n",
       " '169, 67.19, 126.95',\n",
       " '170, 65.99, 111.27',\n",
       " '171, 69.43, 122.61',\n",
       " '172, 67.97, 124.21',\n",
       " '173, 67.76, 124.65',\n",
       " '174, 65.28, 119.52',\n",
       " '175, 73.83, 139.30',\n",
       " '176, 66.81, 104.83',\n",
       " '177, 66.89, 123.04',\n",
       " '178, 65.74, 118.89',\n",
       " '179, 65.98, 121.49',\n",
       " '180, 66.58, 119.25',\n",
       " '181, 67.11, 135.02',\n",
       " '182, 65.87, 116.23',\n",
       " '183, 66.78, 109.17',\n",
       " '184, 68.74, 124.22',\n",
       " '185, 66.23, 141.16',\n",
       " '186, 65.96, 129.15',\n",
       " '187, 68.58, 127.87',\n",
       " '188, 66.59, 120.92',\n",
       " '189, 66.97, 127.65',\n",
       " '190, 68.08, 101.47',\n",
       " '191, 70.19, 144.99',\n",
       " '192, 65.52, 110.95',\n",
       " '193, 67.46, 132.86',\n",
       " '194, 67.41, 146.34',\n",
       " '195, 69.66, 145.59',\n",
       " '196, 65.80, 120.84',\n",
       " '197, 66.11, 115.78',\n",
       " '198, 68.24, 128.30',\n",
       " '199, 68.02, 127.47',\n",
       " '200, 71.39, 127.88']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    2.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    2.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
